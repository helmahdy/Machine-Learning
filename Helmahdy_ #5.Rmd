---
title: 'ML #5'
output: html_document
---

```{r}
library(readr)
library(ISLR)
library(tidyverse)
library(factoextra)
library(ggplot2)
library(lattice)
library(caret)
library(cluster)
library(stats)
library(fastDummies)
Cereals <- read_csv("Cereals.csv")
View(Cereals)

```


```{r}
colMeans(is.na(Cereals)*100)  #Calculate missing values per column
Cereals_df <- Cereals[complete.cases(Cereals),]  #Removing all Cereals with missing values
colMeans(is.na(Cereals_df)*100)

rownames(Cereals_df)<- Cereals_df$name  

#Normalizing Numerical Variables using Z-score and convert variable "shelf" to a dummy variable
Numeric_subset <- Cereals_df[, c(4:16)]
Numeric_subset<- dummy_columns( Numeric_subset, select_columns = "shelf") 

Cerealsdf_Normalized <- scale(Numeric_subset)
head(Cerealsdf_Normalized)

```

Applying hierarchical clustering to the data using Euclidean distance
```{r}
set.seed(122)
Distance <- dist(Cerealsdf_Normalized, method ="euclidean")
Hierarchical_Clust <- hclust(Distance, method= "complete") #hierarchical clustering with complete linakge(Max distance measure)
 Memb <- cutree(Hierarchical_Clust, k = 10)   #COMPUTING CLUSTER MEMBERSHIP BY “CUTTING” THE DENDROGRAM
Memb
plot(Hierarchical_Clust, cex= 0.6, hang=-1)

# Using single linkage clustering  method resulted in agglomerate coefficient of 0.666
HC_singleL <- agnes(Cerealsdf_Normalized, method= "single")
HC_singleL$ac

# Using Complete linkage clustering  method resulted in agglomerate coefficient of 0.835.
HC_completeL <- agnes(Cerealsdf_Normalized, method= "complete")
HC_completeL$ac 
 
# Using Average linkage clustering  method resulted in agglomerate coefficient of 0.771
HC_averageL <- agnes(Cerealsdf_Normalized, method = "average")
HC_averageL$ac

# Using ward method as clustering  method resulted in the highest agglomerate coefficient 0.916 and hence, this is the best linkage method  
HC_Ward <- agnes(Cerealsdf_Normalized, method = "ward")
HC_Ward$ac

#Plot Clustering Tree(dendrogram) of a Hierarchical Clustering(agnes) using  Ward method. looking at the dendrogram, I believe that number of clusters should be between 6-8, if less it would be too many clusters and we will lose meaningful clusteing and if more we will have too few clusers. 
pltree(HC_Ward,  cex = 0.6, hang =-1,  main= "Dendrogram of agne")

#Choosing a number of clusters relies deeply on the domain knowledge , and that's why a domain expert has to make such decision. But we can use Kmean methods such as elbow method or silhouette to choose the optimal k number.

fviz_nbclust(Cerealsdf_Normalized, kmeans, method= "silhouette") # shows 8 as the optimal K number

#Plotting HC using Ward Method and K =8
Hc_ward_Distance <- hclust(Distance, method= "ward.D")

plot(Hc_ward_Distance, cex = 0.6)
rect.hclust(Hc_ward_Distance, k=8 , border= 1:8)
abline(h = 12, col = 'red')

#showing the 8 clusters 
Clusters <- cutree(Hc_ward_Distance,  k=8)
table(Clusters)

```


comparing kmeans and Hierarchical clustering 
```{r, eval=F, echo= T}
1- In Hierarchical clustering it is easier to understand and  choose the number of cluster by looking at the dendrogram. Kmeans requires the number of clusters to be predetemined.
2- Hierarchical clustering doesn't start at a random start point like kmeans. that's why kmeans yields different results everytime it is re-ran.
3-On the other hand, Kmeans is less computationally intensive than Hierarchical clustering
4-Hierarchical clustering also tends to have low stability. Reordering data
or dropping a few records can lead to a different solution.

```

Checking the clustering stability: we take out 10 records of the data and will run Hierarchical Clustering again 
```{r}
set.seed(143)
#Removing 10 records of data to test the stability of HC
Partition_A <- Cerealsdf_Normalized[1:10,] 
Partition_B <- Cerealsdf_Normalized[11:74,]
HC_ward_PartitionB <- agnes(Partition_B, method= "ward")
HC_ward_PartitionB$ac   #the coefficient changed slightly to .907

#Plotting the dendrogram after removing the part of the data
pltree(HC_ward_PartitionB,  cex = 0.6, hang =-1,  main= "Dendrogram of agne")

#The optimal K number changed from 8 to 10
fviz_nbclust(Partition_B, kmeans, method= "silhouette")


# The Hierarchical clustering has low stability. After removing only 10 records, the results has changed significantly. 
Distance2 <- dist(Partition_B, method = "euclidean")
hclust <- hclust(Distance2, method= "ward.D")

plot(hclust, cex = 0.6)
rect.hclust(Hierarchical_Clust, k=10 , border= 1:7)

```


The elementary public schools would like to choose a set of cereals to include in their daily cafeterias. Every day a different cereal is offered, but all cereals should support a healthy diet. For this goal, you are requested to find a cluster of “healthy cereals.” Should the data be normalized? If not, how should they be used in the cluster analysis?
```{r}
#If we have a look at the distribution of the data, we notice some variables have outliers and some has higher ranges than the others. In our case here, we are trying to find the cluster with the highest nutrition value, and so normalizing the data is important as we don't want the variables with the high ranges to outweigh the others.
boxplot(Numeric_subset)

#adding the cluster membership column to the cereals dataframe
Cereals_clusters <- cbind(Cerealsdf_Normalized, Clusters)
Cereals_clusters <- as.data.frame(Cereals_clusters)

# The fviz_dist() function plots a visual representation
fviz_cluster(list(data= Cereals_clusters, cluster = Clusters))

#Grouping nutrition by clusters
By_cluster <- Cereals_clusters %>%
 group_by(Clusters) %>%
 summarize_if(is.numeric, sum, na.rm=TRUE)

#looking at the clusters, the school should choose cluster 7 as it contains healthier choices for cereals. Cluster 7 is high in protin, less fat and sugar and high vitamins  and also high ratings
Transpose_df <- as.data.frame(t(By_cluster)) %>%
  rename(C1 = V1, C2= V2, C3=V3, C4=V4, C5=V5, C6=V6, C7=V7, C8=V8)

Transpose_df 

```















